# Logstash Pipeline Configuration
# ================================

input {
  # TCP Input for JSON logs
  tcp {
    port => 5000
    codec => json_lines
    tags => ["tcp-input"]
  }
  
  # File Input for application logs
  file {
    path => "/var/log/application/*.log"
    start_position => "beginning"
    sincedb_path => "/usr/share/logstash/sincedb"
    codec => multiline {
      pattern => "^%{TIMESTAMP_ISO8601}"
      negate => true
      what => "previous"
    }
    tags => ["file-input"]
  }
  
  # Beats Input
  beats {
    port => 5044
    tags => ["beats-input"]
  }
  
  # HTTP Input for webhooks
  http {
    port => 8080
    codec => json
    tags => ["http-input"]
  }
}

filter {
  # Parse timestamps
  if [timestamp] {
    date {
      match => ["timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss", "yyyy-MM-dd'T'HH:mm:ss.SSSZ"]
      target => "@timestamp"
    }
  }
  
  # Grok filter for common log formats
  if "file-input" in [tags] {
    grok {
      match => {
        "message" => [
          "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:loglevel} %{GREEDYDATA:message_content}",
          "%{COMMONAPACHELOG}",
          "%{COMBINEDAPACHELOG}"
        ]
      }
      overwrite => ["message"]
    }
  }
  
  # Add GeoIP information
  if [client_ip] {
    geoip {
      source => "client_ip"
      target => "geoip"
      add_field => ["[geoip][coordinates]", "%{[geoip][longitude]}"]
      add_field => ["[geoip][coordinates]", "%{[geoip][latitude]}"]
    }
    mutate {
      convert => ["[geoip][coordinates]", "float"]
    }
  }
  
  # User Agent parsing
  if [user_agent] {
    useragent {
      source => "user_agent"
      target => "ua"
      prefix => "ua_"
    }
  }
  
  # Korean text processing
  if [message] and [lang] == "ko" {
    mutate {
      add_field => { "[@metadata][analyzer]" => "nori_analyzer" }
    }
  }
  
  # Add environment metadata
  mutate {
    add_field => {
      "[@metadata][environment]" => "${ENVIRONMENT:development}"
      "[@metadata][service]" => "${SERVICE_NAME:unknown}"
    }
  }
  
  # Remove unnecessary fields
  mutate {
    remove_field => ["host", "port", "@version"]
  }
}

output {
  # Output to Elasticsearch
  elasticsearch {
    hosts => ["es01:9200", "es02:9200", "es03:9200"]
    index => "logstash-%{[@metadata][environment]}-%{+YYYY.MM.dd}"
    document_type => "_doc"
    user => "${ELASTICSEARCH_USERNAME:elastic}"
    password => "${ELASTICSEARCH_PASSWORD:}"
    
    # ILM Settings
    ilm_enabled => true
    ilm_rollover_alias => "logstash"
    ilm_pattern => "{now/d}-000001"
    ilm_policy => "logstash-policy"
  }
  
  # Debug output to stdout (development only)
  if [@metadata][environment] == "development" {
    stdout {
      codec => rubydebug {
        metadata => true
      }
    }
  }
  
  # Error handling - send failed events to dead letter queue
  if "_grokparsefailure" in [tags] or "_jsonparsefailure" in [tags] {
    elasticsearch {
      hosts => ["es01:9200"]
      index => "failed-events-%{+YYYY.MM.dd}"
      document_type => "_doc"
    }
  }
}