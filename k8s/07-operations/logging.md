# ğŸ“Š Kubernetes Logging ì™„ë²½ ê°€ì´ë“œ

> ğŸ’¡ **ëª©í‘œ**: Kubernetes í™˜ê²½ì—ì„œ íš¨ê³¼ì ì¸ ë¡œê¹… ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ê³  ì¤‘ì•™í™”ëœ ë¡œê·¸ ê´€ë¦¬ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

## ğŸ“š ëª©ì°¨

1. [**ë¡œê¹… ì•„í‚¤í…ì²˜**](#ë¡œê¹…-ì•„í‚¤í…ì²˜)
2. [**ì»¨í…Œì´ë„ˆ ë¡œê·¸ ìˆ˜ì§‘**](#ì»¨í…Œì´ë„ˆ-ë¡œê·¸-ìˆ˜ì§‘)
3. [**EFK Stack êµ¬ì¶•**](#efk-stack-êµ¬ì¶•)
4. [**PLG Stack (Promtail, Loki, Grafana)**](#plg-stack)
5. [**Fluentd ê³ ê¸‰ ì„¤ì •**](#fluentd-ê³ ê¸‰-ì„¤ì •)
6. [**ë¡œê·¸ í•„í„°ë§ê³¼ íŒŒì‹±**](#ë¡œê·¸-í•„í„°ë§ê³¼-íŒŒì‹±)
7. [**Best Practices**](#best-practices)

---

## ğŸ¯ ë¡œê¹… ì•„í‚¤í…ì²˜

### Kubernetes ë¡œê¹… ë ˆë²¨

```mermaid
graph TB
    subgraph "Logging Layers"
        CL[Container Logs] --> NL[Node Logs]
        NL --> CLL[Cluster-level Logs]
        CLL --> CS[Centralized Storage]
        
        subgraph "Log Types"
            AL[Application Logs]
            SL[System Logs]
            AL --> stdout/stderr
            SL --> journald/syslog
        end
    end
```

### ë¡œê¹… íŒ¨í„´

| íŒ¨í„´ | ì„¤ëª… | ì‚¬ìš© ì‚¬ë¡€ |
|------|------|----------|
| **Node-level** | ê° ë…¸ë“œì—ì„œ ë¡œê·¸ ìˆ˜ì§‘ | ì‘ì€ í´ëŸ¬ìŠ¤í„° |
| **Sidecar** | Podë§ˆë‹¤ ë¡œê·¸ ìˆ˜ì§‘ ì»¨í…Œì´ë„ˆ | íŠ¹ë³„í•œ ë¡œê·¸ ì²˜ë¦¬ |
| **DaemonSet** | ë…¸ë“œë‹¹ í•˜ë‚˜ì˜ ìˆ˜ì§‘ê¸° | ëŒ€ê·œëª¨ í´ëŸ¬ìŠ¤í„° |
| **Central Agent** | ì¤‘ì•™ ì§‘ì¤‘ì‹ ìˆ˜ì§‘ | í´ë¼ìš°ë“œ í™˜ê²½ |

---

## ğŸ“ ì»¨í…Œì´ë„ˆ ë¡œê·¸ ìˆ˜ì§‘

### ê¸°ë³¸ ë¡œê·¸ í™•ì¸

```bash
# Pod ë¡œê·¸ ë³´ê¸°
kubectl logs <pod-name>
kubectl logs <pod-name> -c <container-name>
kubectl logs <pod-name> --previous  # ì´ì „ ì»¨í…Œì´ë„ˆ ë¡œê·¸
kubectl logs <pod-name> --tail=100  # ë§ˆì§€ë§‰ 100ì¤„
kubectl logs <pod-name> -f          # ì‹¤ì‹œê°„ ë¡œê·¸
kubectl logs <pod-name> --since=1h  # 1ì‹œê°„ ì´ë‚´ ë¡œê·¸

# Label selectorë¡œ ì—¬ëŸ¬ Pod ë¡œê·¸
kubectl logs -l app=myapp --all-containers=true

# Deploymentì˜ ëª¨ë“  Pod ë¡œê·¸
kubectl logs deployment/myapp-deployment --all-containers=true --tail=50
```

### ë¡œê·¸ ë¡œí…Œì´ì…˜ ì„¤ì •

```yaml
# kubelet ì„¤ì • (--container-log-max-size, --container-log-max-files)
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
containerLogMaxSize: 10Mi
containerLogMaxFiles: 5
```

### Sidecar íŒ¨í„´ìœ¼ë¡œ ë¡œê·¸ ìˆ˜ì§‘

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app-with-sidecar-logging
spec:
  containers:
  # ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
  - name: app
    image: myapp:1.0
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log
  
  # ë¡œê·¸ ìˆ˜ì§‘ ì‚¬ì´ë“œì¹´
  - name: log-forwarder
    image: busybox
    command: ['sh', '-c']
    args:
    - tail -f /logs/app.log | while read line; do
        echo "$(date '+%Y-%m-%d %H:%M:%S') $line";
      done
    volumeMounts:
    - name: shared-logs
      mountPath: /logs
  
  volumes:
  - name: shared-logs
    emptyDir: {}
```

---

## ğŸ”¥ EFK Stack êµ¬ì¶•

### 1. Elasticsearch ì„¤ì¹˜

```yaml
# elasticsearch-values.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: logging

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch
  namespace: logging
spec:
  serviceName: elasticsearch
  replicas: 3
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      initContainers:
      - name: increase-vm-max-map
        image: busybox
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        securityContext:
          privileged: true
      
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
        ports:
        - containerPort: 9200
          name: http
        - containerPort: 9300
          name: transport
        env:
        - name: cluster.name
          value: k8s-logs
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: discovery.seed_hosts
          value: elasticsearch-0.elasticsearch,elasticsearch-1.elasticsearch,elasticsearch-2.elasticsearch
        - name: cluster.initial_master_nodes
          value: elasticsearch-0,elasticsearch-1,elasticsearch-2
        - name: ES_JAVA_OPTS
          value: "-Xms1g -Xmx1g"
        - name: xpack.security.enabled
          value: "false"
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        resources:
          requests:
            cpu: 500m
            memory: 2Gi
          limits:
            cpu: 1000m
            memory: 2Gi
  
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 30Gi

---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  namespace: logging
spec:
  selector:
    app: elasticsearch
  clusterIP: None
  ports:
  - port: 9200
    name: http
  - port: 9300
    name: transport
```

### 2. Fluentd DaemonSet

```yaml
# fluentd-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: logging
data:
  fluent.conf: |
    <source>
      @type tail
      @id in_tail_container_logs
      path /var/log/containers/*.log
      pos_file /var/log/fluentd-containers.log.pos
      tag kubernetes.*
      read_from_head true
      <parse>
        @type multi_format
        <pattern>
          format json
          time_format %Y-%m-%dT%H:%M:%S.%NZ
        </pattern>
        <pattern>
          format regexp
          expression /^(?<time>.+) (?<stream>stdout|stderr) (?<flags>[^ ]*) (?<message>.*)$/
          time_format %Y-%m-%dT%H:%M:%S.%N%:z
        </pattern>
      </parse>
    </source>
    
    # Kubernetes ë©”íƒ€ë°ì´í„° ì¶”ê°€
    <filter kubernetes.**>
      @type kubernetes_metadata
      @id filter_kube_metadata
      kubernetes_url "#{ENV['FLUENT_FILTER_KUBERNETES_URL'] || 'https://' + ENV.fetch('KUBERNETES_SERVICE_HOST') + ':' + ENV.fetch('KUBERNETES_SERVICE_PORT') + '/api'}"
      verify_ssl "#{ENV['KUBERNETES_VERIFY_SSL'] || true}"
      ca_file "#{ENV['KUBERNETES_CA_FILE']}"
      skip_labels "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_LABELS'] || 'false'}"
      skip_container_metadata "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_CONTAINER_METADATA'] || 'false'}"
      skip_master_url "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_MASTER_URL'] || 'false'}"
      skip_namespace_metadata "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_NAMESPACE_METADATA'] || 'false'}"
    </filter>
    
    # ì‹œìŠ¤í…œ ë¡œê·¸ ì œì™¸
    <filter kubernetes.**>
      @type grep
      <exclude>
        key $.kubernetes.namespace_name
        pattern ^(kube-system|kube-public|kube-node-lease)$
      </exclude>
    </filter>
    
    # Elasticsearchë¡œ ì „ì†¡
    <match kubernetes.**>
      @type elasticsearch
      @id out_es
      @log_level info
      include_tag_key true
      host elasticsearch.logging.svc.cluster.local
      port 9200
      logstash_format true
      logstash_prefix kubernetes
      logstash_dateformat %Y.%m.%d
      request_timeout 30s
      <buffer>
        @type file
        path /var/log/fluentd-buffers/kubernetes.system.buffer
        flush_mode interval
        retry_type exponential_backoff
        flush_thread_count 2
        flush_interval 5s
        retry_forever
        retry_max_interval 30
        chunk_limit_size 2M
        queue_limit_length 8
        overflow_action block
      </buffer>
    </match>

---
# fluentd-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: logging
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      k8s-app: fluentd-logging
  template:
    metadata:
      labels:
        k8s-app: fluentd-logging
    spec:
      serviceAccountName: fluentd
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1.16-debian-elasticsearch8-1
        env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch.logging.svc.cluster.local"
        - name: FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        - name: FLUENT_ELASTICSEARCH_SCHEME
          value: "http"
        - name: FLUENT_UID
          value: "0"
        - name: FLUENTD_SYSTEMD_CONF
          value: disable
        - name: FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX
          value: "kubernetes"
        - name: FLUENT_ELASTICSEARCH_LOGSTASH_DATEFORMAT
          value: "%Y.%m.%d"
        volumeMounts:
        - name: config
          mountPath: /fluentd/etc
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        resources:
          limits:
            memory: 500Mi
          requests:
            cpu: 100m
            memory: 200Mi
      volumes:
      - name: config
        configMap:
          name: fluentd-config
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd
  namespace: logging

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluentd
rules:
- apiGroups: [""]
  resources:
  - pods
  - namespaces
  verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fluentd
roleRef:
  kind: ClusterRole
  name: fluentd
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: fluentd
  namespace: logging
```

### 3. Kibana ì„¤ì¹˜

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
    spec:
      containers:
      - name: kibana
        image: docker.elastic.co/kibana/kibana:8.11.0
        ports:
        - containerPort: 5601
        env:
        - name: ELASTICSEARCH_HOSTS
          value: http://elasticsearch:9200
        - name: SERVER_HOST
          value: "0.0.0.0"
        resources:
          requests:
            cpu: 100m
            memory: 512Mi
          limits:
            cpu: 500m
            memory: 1Gi

---
apiVersion: v1
kind: Service
metadata:
  name: kibana
  namespace: logging
spec:
  type: LoadBalancer
  selector:
    app: kibana
  ports:
  - port: 5601
    targetPort: 5601
```

---

## ğŸ“Š PLG Stack

### Promtail, Loki, Grafana ìŠ¤íƒ

```yaml
# loki-stack.yaml
# Loki ì„¤ì¹˜
apiVersion: v1
kind: ConfigMap
metadata:
  name: loki-config
  namespace: logging
data:
  loki.yaml: |
    auth_enabled: false
    server:
      http_listen_port: 3100
    ingester:
      lifecycler:
        address: 127.0.0.1
        ring:
          kvstore:
            store: inmemory
          replication_factor: 1
    schema_config:
      configs:
        - from: 2020-10-24
          store: boltdb-shipper
          object_store: filesystem
          schema: v11
          index:
            prefix: index_
            period: 24h
    storage_config:
      boltdb_shipper:
        active_index_directory: /loki/boltdb-shipper-active
        cache_location: /loki/boltdb-shipper-cache
        cache_ttl: 24h
      filesystem:
        directory: /loki/chunks
    limits_config:
      enforce_metric_name: false
      reject_old_samples: true
      reject_old_samples_max_age: 168h

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: loki
  namespace: logging
spec:
  serviceName: loki
  replicas: 1
  selector:
    matchLabels:
      app: loki
  template:
    metadata:
      labels:
        app: loki
    spec:
      containers:
      - name: loki
        image: grafana/loki:2.9.0
        args:
        - -config.file=/etc/loki/loki.yaml
        ports:
        - containerPort: 3100
          name: http
        volumeMounts:
        - name: config
          mountPath: /etc/loki
        - name: storage
          mountPath: /loki
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
      volumes:
      - name: config
        configMap:
          name: loki-config
  volumeClaimTemplates:
  - metadata:
      name: storage
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi

---
# Promtail DaemonSet
apiVersion: v1
kind: ConfigMap
metadata:
  name: promtail-config
  namespace: logging
data:
  promtail.yaml: |
    server:
      http_listen_port: 9080
      grpc_listen_port: 0
    
    positions:
      filename: /tmp/positions.yaml
    
    clients:
      - url: http://loki:3100/loki/api/v1/push
    
    scrape_configs:
    - job_name: kubernetes-pods
      kubernetes_sd_configs:
      - role: pod
      pipeline_stages:
      - docker: {}
      relabel_configs:
      - source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: __host__
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: pod
      - action: replace
        source_labels:
        - __meta_kubernetes_container_name
        target_label: container
      - replacement: /var/log/pods/*$1/*.log
        separator: /
        source_labels:
        - __meta_kubernetes_pod_uid
        - __meta_kubernetes_container_name
        target_label: __path__

---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: promtail
  namespace: logging
spec:
  selector:
    matchLabels:
      name: promtail
  template:
    metadata:
      labels:
        name: promtail
    spec:
      serviceAccountName: promtail
      containers:
      - name: promtail
        image: grafana/promtail:2.9.0
        args:
        - -config.file=/etc/promtail/promtail.yaml
        volumeMounts:
        - name: config
          mountPath: /etc/promtail
        - name: varlog
          mountPath: /var/log
          readOnly: true
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
      volumes:
      - name: config
        configMap:
          name: promtail-config
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
```

---

## ğŸ”§ Fluentd ê³ ê¸‰ ì„¤ì •

### ë©€í‹°ë¼ì¸ ë¡œê·¸ ì²˜ë¦¬

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-multiline
  namespace: logging
data:
  fluent.conf: |
    <source>
      @type tail
      path /var/log/containers/*.log
      pos_file /var/log/fluentd-multiline.log.pos
      tag kubernetes.*
      <parse>
        @type multiline
        format_firstline /^\d{4}-\d{2}-\d{2}/
        format1 /^(?<time>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) \[(?<level>\w+)\] (?<message>.*)$/
        time_format %Y-%m-%d %H:%M:%S
      </parse>
    </source>
    
    # Java Stack Trace ì²˜ë¦¬
    <filter kubernetes.**>
      @type concat
      key message
      multiline_start_regexp /^\d{4}-\d{2}-\d{2}|^[A-Z][a-z]+Exception:/
      multiline_end_regexp /^\s+at\s+|^\s+\.\.\.\s+\d+\s+more/
    </filter>
```

### ë¡œê·¸ ë³€í™˜ ë° í•„í„°ë§

```yaml
# ë¡œê·¸ ë ˆë²¨ë³„ í•„í„°ë§
<filter kubernetes.**>
  @type grep
  <regexp>
    key log_level
    pattern /^(ERROR|WARN|INFO)$/
  </regexp>
</filter>

# ë¡œê·¸ í¬ë§· ë³€í™˜
<filter kubernetes.**>
  @type record_transformer
  enable_ruby true
  <record>
    hostname ${hostname}
    timestamp ${time}
    environment production
    cluster_name my-k8s-cluster
    log_level ${record["log"].match(/ERROR|WARN|INFO|DEBUG/) ? $& : "UNKNOWN"}
  </record>
</filter>

# ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹
<filter kubernetes.**>
  @type record_modifier
  <replace>
    key message
    expression /password=\S+/
    replace password=***MASKED***
  </replace>
  <replace>
    key message
    expression /token=\S+/
    replace token=***MASKED***
  </replace>
</filter>
```

---

## ğŸ¨ ë¡œê·¸ í•„í„°ë§ê³¼ íŒŒì‹±

### êµ¬ì¡°í™”ëœ ë¡œê·¸ íŒŒì‹±

```yaml
# JSON ë¡œê·¸ íŒŒì‹±
<filter kubernetes.**>
  @type parser
  key_name log
  reserve_data true
  <parse>
    @type json
  </parse>
</filter>

# nginx ì•¡ì„¸ìŠ¤ ë¡œê·¸ íŒŒì‹±
<filter kubernetes.nginx.**>
  @type parser
  key_name log
  <parse>
    @type nginx
    expression /^(?<remote>[^ ]*) (?<host>[^ ]*) (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?<method>\S+)(?: +(?<path>[^\"]*?)(?: +\S*)?)?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>[^\"]*)" "(?<agent>[^\"]*)")?$/
    time_format %d/%b/%Y:%H:%M:%S %z
  </parse>
</filter>

# Apache ë¡œê·¸ íŒŒì‹±
<filter kubernetes.apache.**>
  @type parser
  key_name log
  <parse>
    @type apache2
  </parse>
</filter>
```

### ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ ë¼ìš°íŒ…

```yaml
# ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë³„ ë¼ìš°íŒ…
<match kubernetes.production.**>
  @type elasticsearch
  host elasticsearch.logging.svc
  port 9200
  index_name production-logs
  <buffer>
    @type memory
    flush_interval 10s
  </buffer>
</match>

<match kubernetes.staging.**>
  @type elasticsearch
  host elasticsearch.logging.svc
  port 9200
  index_name staging-logs
  <buffer>
    @type memory
    flush_interval 10s
  </buffer>
</match>

# ì• í”Œë¦¬ì¼€ì´ì…˜ë³„ ë¼ìš°íŒ…
<match kubernetes.**app-frontend**>
  @type s3
  aws_key_id YOUR_AWS_KEY
  aws_sec_key YOUR_AWS_SECRET
  s3_bucket frontend-logs
  s3_region us-west-2
  path logs/frontend/%Y/%m/%d/
  <buffer>
    @type file
    path /var/log/fluentd-buffers/s3.buffer
  </buffer>
</match>
```

---

## âœ… Best Practices

### 1. ë¡œê·¸ í‘œì¤€í™”

```json
// í‘œì¤€ ë¡œê·¸ í¬ë§·
{
  "timestamp": "2024-01-01T10:00:00Z",
  "level": "INFO",
  "service": "user-service",
  "trace_id": "abc123",
  "span_id": "def456",
  "user_id": "user123",
  "message": "User login successful",
  "metadata": {
    "ip": "192.168.1.1",
    "user_agent": "Mozilla/5.0"
  }
}
```

### 2. ë¡œê·¸ ë ˆë²¨ ê°€ì´ë“œë¼ì¸

| ë ˆë²¨ | ì‚¬ìš© ì‹œì  | ì˜ˆì‹œ |
|------|----------|------|
| **DEBUG** | ê°œë°œ/ë””ë²„ê¹… ì •ë³´ | ë³€ìˆ˜ ê°’, ìƒíƒœ ë³€í™” |
| **INFO** | ì¼ë°˜ ì •ë³´ | ìš”ì²­ ì²˜ë¦¬, ì‘ì—… ì™„ë£Œ |
| **WARN** | ê²½ê³  ìƒí™© | ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©, ëŠë¦° ì¿¼ë¦¬ |
| **ERROR** | ì˜¤ë¥˜ ë°œìƒ | ì˜ˆì™¸ ì²˜ë¦¬, ì‹¤íŒ¨í•œ ì‘ì—… |
| **FATAL** | ì¹˜ëª…ì  ì˜¤ë¥˜ | ì„œë¹„ìŠ¤ ì¤‘ë‹¨, ë°ì´í„° ì†ì‹¤ |

### 3. ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

```yaml
# ë¯¼ê° ì •ë³´ í•„í„°ë§
<filter kubernetes.**>
  @type record_modifier
  <replace>
    key message
    expression /(password|token|secret|key)=\S+/i
    replace \1=***REDACTED***
  </replace>
</filter>

# íŠ¹ì • ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì œì™¸
<filter kubernetes.**>
  @type grep
  <exclude>
    key $.kubernetes.namespace_name
    pattern ^(secrets|sensitive)$
  </exclude>
</filter>
```

### 4. ì„±ëŠ¥ ìµœì í™”

```yaml
# ë²„í¼ ì„¤ì • ìµœì í™”
<buffer>
  @type file
  path /var/log/fluentd-buffers/
  flush_mode interval
  flush_interval 10s
  flush_at_shutdown true
  retry_type exponential_backoff
  retry_forever false
  retry_max_interval 30
  chunk_limit_size 5M
  queue_limit_length 10
  overflow_action drop_oldest_chunk
</buffer>

# ë¡œê·¸ ìƒ˜í”Œë§
<filter kubernetes.**>
  @type sampling
  interval 10  # 10ê°œ ì¤‘ 1ê°œë§Œ ìˆ˜ì§‘
</filter>
```

### 5. ëª¨ë‹ˆí„°ë§

```yaml
# Prometheus ë©”íŠ¸ë¦­
apiVersion: v1
kind: Service
metadata:
  name: fluentd-metrics
  namespace: logging
  labels:
    app: fluentd
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "24231"
spec:
  ports:
  - name: metrics
    port: 24231
    targetPort: 24231
  selector:
    k8s-app: fluentd-logging
```

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¡œê·¸ ìˆ˜ì§‘ ì•ˆë¨

```bash
# Fluentd ìƒíƒœ í™•ì¸
kubectl logs -n logging daemonset/fluentd

# íŒŒì¼ ê¶Œí•œ í™•ì¸
kubectl exec -n logging fluentd-xxx -- ls -la /var/log/containers/

# Buffer ìƒíƒœ í™•ì¸
kubectl exec -n logging fluentd-xxx -- ls -la /var/log/fluentd-buffers/
```

### Elasticsearch ì—°ê²° ì‹¤íŒ¨

```bash
# Elasticsearch ìƒíƒœ í™•ì¸
kubectl get pods -n logging | grep elasticsearch
kubectl logs -n logging elasticsearch-0

# ë„¤íŠ¸ì›Œí¬ ì—°ê²° í…ŒìŠ¤íŠ¸
kubectl exec -n logging fluentd-xxx -- curl elasticsearch:9200/_cluster/health
```

### ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©

```yaml
# ë©”ëª¨ë¦¬ ì œí•œ ë° ë²„í¼ í¬ê¸° ì¡°ì •
resources:
  limits:
    memory: 1Gi
  requests:
    memory: 512Mi

<buffer>
  chunk_limit_size 2M  # ì²­í¬ í¬ê¸° ì¤„ì´ê¸°
  queue_limit_length 4  # í ê¸¸ì´ ì¤„ì´ê¸°
</buffer>
```

---

## ğŸ’¡ ê³ ê¸‰ íŒ

### 1. ë©€í‹° í´ëŸ¬ìŠ¤í„° ë¡œê¹…

```yaml
# í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¶”ê°€
<filter kubernetes.**>
  @type record_transformer
  <record>
    cluster_name ${ENV["CLUSTER_NAME"]}
    region ${ENV["AWS_REGION"]}
  </record>
</filter>
```

### 2. ë¡œê·¸ ì•„ì¹´ì´ë¹™

```bash
# S3ë¡œ ì˜¤ë˜ëœ ë¡œê·¸ ì•„ì¹´ì´ë¹™
curl -XPUT "elasticsearch:9200/_ilm/policy/archive-policy" -H 'Content-Type: application/json' -d'
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_age": "7d",
            "max_size": "50GB"
          }
        }
      },
      "warm": {
        "min_age": "7d",
        "actions": {
          "shrink": {
            "number_of_shards": 1
          }
        }
      },
      "cold": {
        "min_age": "30d",
        "actions": {
          "freeze": {}
        }
      },
      "delete": {
        "min_age": "90d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}'
```

### 3. ì‹¤ì‹œê°„ ë¡œê·¸ ìŠ¤íŠ¸ë¦¬ë°

```bash
# stern ì‚¬ìš© (ì—¬ëŸ¬ Pod ë¡œê·¸ ë™ì‹œ í™•ì¸)
stern -n production "app-.*" --tail 100

# kubetail ì‚¬ìš©
kubetail -n production -l app=myapp
```

---

> ğŸš€ íš¨ê³¼ì ì¸ ë¡œê¹… ì‹œìŠ¤í…œìœ¼ë¡œ ìš´ì˜ ê°€ì‹œì„±ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤!